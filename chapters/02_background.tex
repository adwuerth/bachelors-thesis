\chapter{Background}

\section{vroom}
Vroom is a userspace NVMe driver written in Rust. While it is not production ready at the time of writing, it already provides good performance and the functionality needed for general I/O operations.
A NVMe driver consists of submission and completion queues, implemented as ring buffers. The driver adds commands to the submission queue, which the NVMe controller reads and executes. The executed command gets placed on a corresponding completion queue.
We unbind the kernel driver and bind it to pci-stub. Pci-stub does not do anything but occupy the pci-driver such that the kernel or another application can not bind to the device.

\section{Memory Mapped I/O}
vroom currently uses hugepages and locks them using mlock to prevent the Kernel from swapping them out. In order to 'pin' pages, preventing the Linux Kernel from switching them around, Hugepages have to be used. The kernel does not have the ability to move these pages like 4KiB pages.
For accessing the devices memory as well as the device accessing the host memory, it is necessary to either use the physical addresses and compromise on safety and use root privileges or use the IOMMU for virtualization, which can introduce performance overhead.


\section{Peripheral Component Interconnect Express}

\section{Direct Memory Access}
Using Direct Memory Access we can bypass the CPU for I/O operations. Previously this was handled by a separate DMA-controller hardware (third-party DMA) but using PCI, we can directly access it through bus mastering (first-party DMA).

\section{I/O Memory Management Unit}
Memory Management Units (MMU) for the CPU have been in use since the 1980s. After their first integrated application featuring on Intels 80286 chip \cite{intel80286}, they since have become the defacto standard for addressing memory on computers. By providing processes with a virtual address space instead of physical addresses, every process is isolated and can not access without having the privilege to. The MMU uses pages for the translation of addresses. Each address points to a region of memory called a page. These pages can have different sizes, with the default being 4Kib pages.
The translation of these pages are stored in a page table structure. A page table structure consists of multiple tables that store parts of the physical address. Certain parts of an address are used as offset in these tables. When an address is translated, a page table walk has to performed. On a 4 level page table structure as the IOMMU uses for 4KiB pages, one address resolution uses 4 memory accesses. Thus, a page table walk is a performance costly operation. To circumvent this, there exists an Translation Lookaside Buffer (TLB). This TLB can store a certain amount of page translations, and is very performant to access. Frequent access to the same address can be done at a fraction of the time needed to perform a page table walk. If an address is not stored in the TLB, it is called a TLB miss, and the IOMMU has to perform a page table walk.
The advantages and success of the CPUs MMU as well as the introduction of the PCIe bus specification have incentivised Hardware manufacturers to apply this concept on peripheral device busses. In 2006, Intel introduced their "Virtualization Technology for Directed I/O" (VT-d) and AMD their "AMD I/O Virtualization Technology" (AMD-Vi/IOMMU). In this thesis, the term IOMMU references both technologies.
Using DMA Remapping (DMAR) the CPU is bypassed and the direct memory access translated by the IOMMU.
The IOMMU paging structures are each 512 * 8 Bytes = 4kib in size. The IOMMU uses the upper portions to determine the location of the stored page tables, and the lower portion of the address as page offset. In the case of 4KiB pages its 12 bits as 2\^12 = 4096 = 4Kib, for 2MiB its 21 as 2\^21 = 2097152 = 2Mib.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/4kibtranslation.pdf}
    \caption{VT-d Paging structure for translating a 48-bit address to a 4-KByte page}
    \label{fig:pagewalk4kib}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/2mibtranslation.pdf}
    \caption{VT-d Paging structure for translating a 48-bit address to a 2-MByte page}
    \label{fig:pagewalk2mib}
\end{figure}


\section{Hugepages}
As the demand for bigger memory mappings e.g. for big files increased, the amount of TLB cache misses rose proportionally. With the CPUs TLB having space for only 4096 4KiB pages, only an address space of 16MiB could be stored and accessed quickly. In order to increase the amount of virtual memory space, hardware producers reacted by providing bigger page sizes on their architectures than the default 4KiB. x86 architectures now normally provide 4K, 2M and 1G page sizes.
Linux currently provides two ways of using Hugepages.
Using a 2MiB or 1GiB page size should result in a 512 or 512*512=262144 times reduction in cache misses compared to 4KiB pages. Especially in high-performance computing this make a huge difference.

\begin{itemize}
    \item \textbf{Persistent Hugepages}: Persistent Hugepages, are reserved in the kernel and cannot be swapped out or used for another purpose. To use these hugepages, they are mounted as a (pseudo) filesystem called hugeltbfs, which lays in the directory \texttt{/mnt/huge}. \cite{hugetlbkerneldocs} The amount and size of the pages can be specified either during boot on the kernel commandline with e.g. \texttt{hugepagesz=1g hugepages=16} or dynamically using the Linux \texttt{sysfs} pseudofilesystem e.g. \texttt{echo 16 > /proc/sys/vm/nr\_hugepages}
    \item \textbf{Transparent Hugepages}: Transparent Hugepages are a more recent addition to the kernel. Transparent hugepages are not fixed or reserved in the kernel and allow all unused memory to be used for other purposes. The \texttt{khugepaged} daemon scans memory and collapses sequences of basic pages into bigger pages \cite{transhugekerneldocs}. THPs can either be enabled, disabled or only be used on \texttt{madvise(MADV\_HUGEPAGE)} memory regions.
\end{itemize}

\section{I/O Translation Lookaside Buffer}
As page table walks are rather costly in performance, a cache on the IOMMU is used to store previously calculated addresses. This cache is called the Input/Output Translation Lookaside Buffer. The IOTLB possesses a limited capacity for entries which is not officially documented.

\subsection{Character and Block Devices}
Unix/Linux use two types of devices: Character and Block devices. Character devices are used for devices with small amounts of data and no frequent seek queries, like keyboard and mouse. Block devices on the other hand have a large data volumes, which are organized in blocks and where search is common, like harddrives and ram disks.
Read and Write operations on character devices are done sequentially byte-by-byte, while on block devices, read/write is done at the data block level.
These constraints also impact how the drivers for these devices work. CDev drivers directly communicate with the device drivers, while block device drivers work in conjunction with the kernel file management and block device subsystem. This allows efficient asynchronous read/write operations for large data amounts, but small byte sized data transfer achieves lower latency on character devices.

\subsection{Rust}
Rust as a programming language offers a lot of benefits, especially in the systems programming field. The memory safety enforced by the borrow checker and the focus on providing the most concise and exact syntax like default variable immutability while obmitting boilerplate make it an excellent choice for modern systems development.