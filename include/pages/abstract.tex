\chapter{Abstract}

Peripheral devices like SSDs need access to main memory in order to perform I/O operations. High-performance drivers take advantage of Direct Memory Access (DMA), a feature that allows devices to access system memory independently of the CPU. This can be a huge security risk as malicious firmware attacks or faulty operations could lead to detrimental consequences, potentially extracting data or corrupting the system. The workaround to this is the IOMMU (Input-Output Memory Management Unit), which maps physical to I/O virtual addresses, similar to the CPU's MMU, providing access rights enforcement. As address translation can be a memory- and performance-intensive operation, it is necessary to examine how impactful the IOMMU is on the whole driver's performance.
In this thesis, we implement IOMMU support for a userspace NVMe driver written in Rust and examine its performance, directly comparing DMA performance with physical addresses and IOMMU I/O virtual addresses. We demonstrate that essentially identical performance may be achieved with \qty{2}{\mebi\byte} pages, along with enhanced system security and the ability to run the driver without root privileges. We also add support for the new user API IOMMUFD, which can be used as a modern backend for VFIO.